{"id": "00ae917f-ae9f-49e3-48c8-38c2e9600993", "content": "Facebook afronta otra demanda por orientar sus anuncios de forma discriminatoria\n\nCuando Neutah Opiotennione, una residente de Washington DC de 54 a\u00f1os, descubri\u00f3 que Facebook le ocultaba algunos anuncios sobre productos financieros por ser una mujer mayor, decidi\u00f3 que deb\u00eda hacer algo al respecto. El pasado mes de octubre, Opiotennione se uni\u00f3 a otros usuarios de la red social para presentar una demanda colectiva por la discriminaci\u00f3n que sufren debido a la herramienta de anuncios que utiliza la plataforma.\nLos demandantes afirman que Facebook permite a los anunciantes discriminar por edad y g\u00e9nero a la ahora de ofrecer publicidad de ciertos servicios financieros, tales como cuentas bancarias, seguros, inversiones o pr\u00e9stamos, ya que este tipo de anuncios no le aparecen a mujeres y personas mayores con la misma frecuencia que al resto de los usuarios. Seg\u00fan la demanda, Facebook persiste en este tipo de discriminaci\u00f3n, a pesar de que la compa\u00f1\u00eda asegur\u00f3 hace unos meses que tomar\u00eda medidas al respecto.\nNo es la primera vez que la red social es criticada por este motivo. El pasado mes de marzo, el Gobierno federal acus\u00f3 a la compa\u00f1\u00eda por discriminaci\u00f3n en la publicidad relacionada con la vivienda. \"Facebook est\u00e1 discriminando a las personas en base a qui\u00e9nes son y d\u00f3nde viven\", asegur\u00f3 el secretario del Departamento de Vivienda y Desarrollo Urbano, Ben Carson, en un comunicado.\nLa situaci\u00f3n salt\u00f3 tras una investigaci\u00f3n publicada por el medio estadounidense ProPublica en 2016, que encontr\u00f3 que una herramienta de Facebook pod\u00eda utilizarse para excluir a los usuarios negros y latinos de que se les mostraran ciertos anuncios inmobiliarios, algo que no permite la ley. Seg\u00fan la Ley Federal de Vivienda Justa, es ilegal publicar un anuncio \"con respecto a la venta o alquiler de una vivienda que indique cualquier preferencia, limitaci\u00f3n o discriminaci\u00f3n por motivos de raza, color, religi\u00f3n, sexo, discapacidad, estado familiar u origen nacional\u201d.\nSeg\u00fan la Ley Federal de Vivienda Justa, es ilegal publicar un anuncio \"con respecto a la venta o alquiler de una vivienda que indique cualquier preferencia, limitaci\u00f3n o discriminaci\u00f3n por motivos de raza, color, religi\u00f3n, sexo, discapacidad, estado familiar u origen nacional\u201d\nLa red social anunci\u00f3 entonces que tomar\u00eda medidas al respecto, pero, un a\u00f1o m\u00e1s tarde, una nueva investigaci\u00f3n del medio norteamericano mostr\u00f3 que las medidas de control a\u00fan no eran del todo eficaces.\nAhora, los demandantes alegan que la empresa sigue permitiendo que las compa\u00f1\u00edas de servicios financieros filtren los anuncios por edad y g\u00e9nero, lo que supondr\u00eda una violaci\u00f3n de la ley de derechos civiles de California. \"Internet no es un lugar donde se pueda discriminar a las personas por su edad o g\u00e9nero, particularmente en las oportunidades de servicios financieros\", asegur\u00f3 a Reuters Peter Romer-Friedman, representante de los demandantes. \"Ser\u00eda como si General Motors se negara a ofrecer a las mujeres o a los mayores las mismas caracter\u00edsticas en un coche que a los hombres o a los j\u00f3venes\u201d.\nLa demanda se ha producido en medio de un debate sobre c\u00f3mo la actual legislaci\u00f3n norteamericana protege a los ciudadanos no solo de la discriminaci\u00f3n directa, como la que se produce en el caso de los anuncios de Facebook, sino tambi\u00e9n de la generada por la inteligencia artificial, cuyos efectos de segregaci\u00f3n son similares, pero cuyo origen es m\u00e1s dif\u00edcil de rastrear.\nLa actual legislaci\u00f3n estadounidense establece que incluso las exclusiones no intencionadas, como las generadas por una inteligencia artificial, pueden ser penadas, con lo que una aplicaci\u00f3n de alquiler de vivienda cuyo algoritmo excluya a las minor\u00edas podr\u00eda considerarse ilegal. Sin embargo, el pasado mes de agosto se plante\u00f3 una reforma para que solo que se considerasen punibles aquellas que introducen directamente par\u00e1metros discriminatorios en el algoritmo.\nEl problema, seg\u00fan los expertos, es que la no inclusi\u00f3n de ciertos par\u00e1metros en un algoritmo no impide que el resultado sea discriminatorio, como demostr\u00f3 recientemente un estudio que determin\u00f3 que un algoritmo usado para analizar los riesgos para la salud de millones de pacientes en EE UU discrimina sistem\u00e1ticamente a la poblaci\u00f3n negra, a pesar de que el sistema desconoc\u00eda la raza de los afectados.\nTambi\u00e9n la reciente pol\u00e9mica de la Apple Card, cuyos algoritmos pueden llegar a ofrecer 20 veces menos cr\u00e9dito a una mujer que un hombre en una situaci\u00f3n econ\u00f3mica similar, es un ejemplo de c\u00f3mo la ausencia de ciertos datos de entrada no implica que el resultado no sea discriminatorio, ya que Goldman Sachs, banco con el que Apple ha sacado su tarjeta de cr\u00e9dito, ha asegurado que en los datos que se introducen en el sistema no se incluye el g\u00e9nero o el estado civil de los usuarios.\nSeg\u00fan un estudio publicado a principios de este a\u00f1o en arXiv sobre los problemas generados por los algoritmos, esta discriminaci\u00f3n de salida se puede producir porque \u201clos datos son un producto de muchos factores, desde el contexto hist\u00f3rico en el que fueron generados hasta las formas particulares de error de medici\u00f3n que contienen\u201d, por lo que pueden contener sesgos impl\u00edcitos que no son f\u00e1ciles de identificar a simple vista.\nEn este sentido, un estudio publicado en 2017 en la revista Science mostr\u00f3 que cuando una inteligencia artificial aprende un idioma, \u00e9sta termina por asumir los sesgos humanos que est\u00e1n incluidos impl\u00edcitamente en el lenguaje, reproduciendo comportamientos que pueden ser machistas y racistas.\nEl problema no se encuentra solo en las caracter\u00edsticas m\u00e1s menos sesgadas de los datos de entrada, sino en el hecho de que el propio sistema de aprendizaje autom\u00e1tico tambi\u00e9n puede generar sesgos, ya que \u201cincluye una serie de opciones y pr\u00e1cticas, desde la metodolog\u00eda de evaluaci\u00f3n hasta la definici\u00f3n del modelo, que pueden conducir a efectos no deseados\u201d.\n\u201cEl aprendizaje autom\u00e1tico no solo se basa en la muestra de entrada, sino en las decisiones se toman en base a esos datos\u201d, afirma Liliana Arroyo, investigadora del Instituto de Innovaci\u00f3n Social de ESADE. \u201cComo es algo automatizado creemos que son neutros, pero la realidad es que los algoritmos son ideolog\u00eda\u201d.\nLa importancia de c\u00f3mo legislar sobre estos algoritmos, que tienden a reproducir los sesgos de los seres humanos, es vital, ya que son \u00e9stos los que deciden no solo qu\u00e9 videos le pueden gustar a un usuario, sino tambi\u00e9n si se le ofrece o no un cr\u00e9dito bancario, una vivienda o un seguro m\u00e9dico. \u201cLos algoritmos hacen lo que hemos hecho siempre, pero de una forma autom\u00e1tica y a una escala mucho mayor, por lo que el impacto que tienen es mucho m\u00e1s grande\u201d, concluye Arroyo.\n NEWSLETTER \n Recibe la mejor informaci\u00f3n en tu bandeja de entrada \nUna breve gu\u00eda para que el hombre moderno (o sea, t\u00fa) sepa c\u00f3mo regalar joyas estas navidades evitando compras fallidas de \u00faltima hora y triste desaciertos. Si ya no triunfas regalando, es porque no quieres", "date": "11/18/2019, 20:01:45", "tags": ["Facebook", "Publicidad", "Redes sociales", "Internet", "Empresas", "Econom\u00eda", "Telecomunicaciones", "Tecnolog\u00eda", "Medios comunicaci\u00f3n", "Comunicaciones", "Comunicaci\u00f3n", "Ciencia"], "newspaper": "elpais"}