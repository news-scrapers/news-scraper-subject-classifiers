{"id": "e92fdcd2-93c2-43b1-7c63-44fcf36d3324", "content": "El algoritmo que discrimina a los pacientes negros sin conocer su raza\n\nHubo un tiempo en el que se pens\u00f3 que los algoritmos ayudar\u00edan a tomar decisiones sin los prejuicios que a veces nublan la mente de los humanos. El rigor de las matem\u00e1ticas y la frialdad de los datos resolver\u00edan de forma autom\u00e1tica muchos de nuestros problemas. Pero innumerables ejemplos, en campos tan sensibles como el trabajo, el crimen o la inmigraci\u00f3n, han demostrado que ni siquiera los programas m\u00e1s sofisticados evitan estos problemas. Los algoritmos tienen sesgos, es inevitable, porque las m\u00e1quinas se nutren con informaci\u00f3n humana. Ahora, un equipo de investigadores ha demostrado que un algoritmo usado para analizar los riesgos para la salud de millones de pacientes en EE UU discrimina sistem\u00e1ticamente a la poblaci\u00f3n negra. Y lo hace sin saber siquiera su raza. Lo peor, advierten los especialistas, es que el mismo mecanismo sesgado se est\u00e1 usando ahora mismo con millones de personas en todo el mundo y en muchos otros \u00e1mbitos.\n\"El algoritmo predice cu\u00e1nto dinero costar\u00e1 un paciente. Y aqu\u00ed es donde entra en juego el sesgo. Esta variable es sistem\u00e1ticamente diferente para los pacientes blancos y negros\", explica Obermeyer\nPara asignar recursos sanitarios de forma eficiente, el algoritmo distribuye los pacientes seg\u00fan el riesgo m\u00e9dico asociado a sus caracter\u00edsticas. As\u00ed, los que tengan peor condici\u00f3n de salud tendr\u00e1n acceso a un programa espec\u00edfico de atenci\u00f3n sanitaria. Sin embargo, tras estudiar concienzudamente los ingredientes que se usan para cocinar esa distribuci\u00f3n de los pacientes, los investigadores han descubierto que los negros est\u00e1n mucho peor que los blancos incluidos juntos en la misma categor\u00eda. Es decir, que habr\u00eda miles de negros que quedar\u00edan fuera del programa estando mucho m\u00e1s enfermos que los blancos. \u201cPodemos cuantificar estas diferencias\u201d, escriben los investigadores en su estudio en la revista Science, \u201cen el grupo de muy alto riesgo los negros tienen 26,3% m\u00e1s enfermedades cr\u00f3nicas que los blancos\u201d. Seg\u00fan datos de la industria, este modelo se aplica a unos 200 millones de personas cada a\u00f1o en EE UU.\nPor primera vez en un algoritmo de esta trascendencia y dimensi\u00f3n, la empresa que lo comercializa permiti\u00f3 a los investigadores tener acceso a todos los ingredientes con el que confecciona su resultado. En todos los factores importantes del estado de salud, como gravedad de la diabetes, presi\u00f3n arterial alta, insuficiencia renal, colesterol y anemia, descubrieron que los negros tienen \u201csustancialmente peor salud que los blancos en cualquier nivel de predicciones del algoritmo\u201d.\nLo m\u00e1s llamativo es que el algoritmo no conoc\u00eda la raza de los pacientes, no era un dato que se le proporcionara para tomar sus decisiones. \u00bfY c\u00f3mo discrimina con esa precisi\u00f3n a los negros? Porque la predicci\u00f3n del algoritmo, descubrieron los cient\u00edficos, no eran las necesidades m\u00e9dicas de los pacientes sino los costes sanitarios. Y los negros son m\u00e1s baratos para el sistema sanitario: a un mismo nivel de salud, los negros son de media 1.800 d\u00f3lares m\u00e1s baratos que los blancos.\n\u201cLa variable m\u00e1s importante para un algoritmo no es ninguno de los inputs, ni siquiera la raza. Es el output: la variable que el algoritmo intenta predecir\u201d, explica el principal autor del estudio, Ziad Obermeyer, de la Universidad de Berkeley (EE UU). \u201cEn este caso, el algoritmo predice cu\u00e1nto dinero costar\u00e1 un paciente. Y aqu\u00ed es donde entra en juego el sesgo. Esta variable, el coste, es sistem\u00e1ticamente diferente para los pacientes blancos y negros: al mismo nivel de salud, los pacientes negros generan menos gastos que los pacientes blancos, debido a las diferencias en el acceso a la atenci\u00f3n y las diferencias en la forma son tratados por el sistema de salud\u201d, desarrolla Obermeyer. \u201cEl problema surge porque la variable en s\u00ed, la que se supone que predice el algoritmo, contiene el sesgo\u201d, resume el investigador.\n\"En muchos casos las empresas no hacen ning\u00fan esfuerzo en comprender las din\u00e1micas sociales que pueden ser negativas, ni piensan que lo necesitan\", denuncia Galdon\nDespu\u00e9s de realizar su an\u00e1lisis, los investigadores contactaron al fabricante. \u201cEstaban bastante sorprendidos y preocupados por nuestros hallazgos, y deseaban corregir el problema. As\u00ed que trabajamos con ellos para probar algunas soluciones diferentes, todas relacionadas con el cambio de la variable que el algoritmo intentaba predecir\u201d, cuenta Obermeyer en un correo electr\u00f3nico. El fabricante replic\u00f3 de forma independiente el an\u00e1lisis con m\u00e1s de tres millones y medio de pacientes y confirmaron los resultados. M\u00e1s adelante, los cient\u00edficos propusieron cambios sobre la forma de alimentar el algoritmo y para que en lugar de predecir solo el gasto en los pacientes se centrara tambi\u00e9n en predecir la salud de los sujetos. Con este enfoque se redujo un 84% el sesgo que perjudicaba a los negros. Los algoritmos, concluyen, tienen arreglo si se planean teniendo en cuenta estos factores.\n\"Cada vez se est\u00e1 prestando m\u00e1s atenci\u00f3n a estos problemas y ser\u00e1 m\u00e1s habitual, porque muchas de las empresas proveedoras de algoritmos no tienen ni siquiera los recursos para identificarlos\", asegura Gemma Galdon, directora de Eticas y especialista en el impacto social de la tecnolog\u00eda. Galdon trabaja habitualmente tratando de corregir estos sesgos con administraciones p\u00fablicas en algoritmos de asignaci\u00f3n de recursos, por ejemplo a familias en el \u00e1mbito de los servicios sociales, algoritmos de aplicaciones de salud, o del \u00e1mbito laboral. \"Es nuestro pan de cada d\u00eda en las auditor\u00edas algor\u00edtmicas: el problema es la definici\u00f3n del problema, hace falta tener una muy buena comprensi\u00f3n del asunto\", afirma. \"En muchos casos los proveedores no hacen ning\u00fan esfuerzo en comprender las din\u00e1micas sociales que pueden ser negativas, ni piensan que lo necesitan\", denuncia Galdon. Y a\u00f1ade: \"Nos encontramos errores muy b\u00e1sicos. En muchos casos las empresas no se preocupan de comprender realmente el problema y usar el conocimiento social que tenemos para resolverlo\".\nTambi\u00e9n en Science, Ruha Benjamin, especialista en innovaci\u00f3n e igualdad de la Universidad de Princeton, asegura que \"este estudio contribuye en gran medida a un enfoque m\u00e1s socialmente consciente para el desarrollo tecnol\u00f3gico\", demostrando c\u00f3mo una elecci\u00f3n aparentemente benigna (gasto en salud) \"inicia un proceso con resultados potencialmente mortales\". A su entender, esta desigualdad tecnol\u00f3gica \"se perpet\u00faa precisamente porque quienes dise\u00f1an y adoptan tales herramientas no est\u00e1n pensando cuidadosamente sobre el racismo sist\u00e9mico\". \"Pero las decisiones humanas conforman los datos y el dise\u00f1o de algoritmos, ahora mismo ocultos por la promesa de neutralidad y con el poder de discriminar injustamente a una escala mucho mayor que unos individuos sesgados\", denuncia Benjamin.\nPuede escribirnos a javier@esmateria.com o seguir a Materia en Facebook, Twitter, Instagram o suscribirse aqu\u00ed a nuestra newsletter.\n NEWSLETTER \n Recibe el bolet\u00edn de Ciencia \nUna breve gu\u00eda para que el hombre moderno (o sea, t\u00fa) sepa c\u00f3mo regalar joyas estas navidades evitando compras fallidas de \u00faltima hora y triste desaciertos. Si ya no triunfas regalando, es porque no quieres", "date": "10/24/2019, 19:01:45", "tags": ["Algoritmos computacionales", "Computaci\u00f3n", "Racismo", "Discriminaci\u00f3n", "Inform\u00e1tica", "Prejuicios", "Problemas sociales", "Tecnolog\u00eda", "Justicia", "Ciencia"], "newspaper": "elpais"}